{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as random\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "%precision 3\n",
    "\n",
    "import sklearn\n",
    "import requests, zipfile\n",
    "import io\n",
    "\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__정규화 선형회귀__\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "정규화(regularized) 선형회귀 방법은 선형회귀 계수(weight)에 대한 제약 조건을 추가함으로써 모형이 과도하게 최적화되는 현상, 즉 과최적화를 막는 방법이다. Regularized Method, Penalized Method, Contrained Least Squares 이라고도 불리운다.\n",
    "\n",
    "모형이 과도하게 최적화되면 모형 계수의 크기도 과도하게 증가하는 경향이 나타난다. 따라서 정규화 방법에서 추가하는 제약 조건은 일반적으로 계수의 크기를 제한하는 방법이다. 일반적으로 다음과 같은 세가지 방법이 사용된다.\n",
    "\n",
    "* Ridge 회귀모형\n",
    "* Lasso 회귀모형\n",
    "* Elastic Net 회귀모형\n",
    "\n",
    "<p>&nbsp;</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ridge 회귀모형__\n",
    "\n",
    "---\n",
    "\n",
    "Ridge 회귀모형에서는 가중치들의 제곱합(squared sum of weights)을 최소화하는 것을 추가적인 제약 조건으로 한다.\n",
    "\n",
    "$$\n",
    "w = \\text{arg}\\min_w \\left( \\sum^N_{i=1}e^2_i + \\lambda \\sum^M_{j=1}w^2_j\\right)\n",
    "$$\n",
    "\n",
    "$\\lambda$는 기존의 잔차 제곱합과 추가적 제약 조건의 비중을 조절하기 위한 하이퍼 모수(hyper parameter)이다. $\\lambda$가 크면 정규화 정도가 커지고 가중치의 값들이 작아진다. $\\lambda$가 작아지면 정규화 정도가 작아지며 $\\lambda$가 0이 되면 일반적인 선형 회귀모형이 된다.\n",
    "\n",
    "<p>&nbsp;<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Lasso 회귀모형__\n",
    "\n",
    "---\n",
    "\n",
    "Lasso(Least Absolute Shrinkage and Selection Operator) 회귀모형은 가중치의 절대값의 합을 최소화하는 것을 추가적인 제약 조건으로 한다.\n",
    "\n",
    "$$\n",
    "w = \\text{arg} \\min_w \\left( \\sum^N_{i=1} e^2_i + \\lambda \\sum^M_{j=1} |w_j| \\right)\n",
    "$$\n",
    "\n",
    "<p>&nbsp;</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Elastic Net 회귀모형__\n",
    "\n",
    "---\n",
    "\n",
    "Elastci Net 회귀모형은 가중치의 절대값의 합과 제곱합을 동시에 제약 조건으로 가지는 모형이다.\n",
    "\n",
    "$$\n",
    "w = \\text{arg} \\min_w \\left( \\sum^N_{i=1} e^2_i + \\lambda_1 \\sum^M_{j=1} |w_j| + \\lambda_2 \\sum^M_{j=1} w^2_j \\right)\n",
    "$$\n",
    "\n",
    "$\\lambda_1$, $\\lambda_2$ 두 개의 하이퍼 모수를 가진다.\n",
    "\n",
    "<p>&nbsp;</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__statsmodels의 정규화 회귀모형__\n",
    "\n",
    "---\n",
    "\n",
    "statsmodels 패키지는 OLS 선형 회귀모형 클래스의 `fit_regularized` 메서드를 사용하여 Elastic Net 모형 계수를 구할 수 있다.\n",
    "\n",
    "하이퍼 모수는 다음과 같이 모수 $\\text{alpha}$와 $\\text{L1} \\_ \\text{wt}$로 정의된다.\n",
    "\n",
    "$$\n",
    "0.5 \\times \\text{RSS} / \\text{alpha} \\times (0.5 \\times (1 - \\text{L1} \\_ \\text{wt}) \\sum w^2_i + \\text{L1} \\_ \\text{wt} \\sum |w_i|)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nonlinear(seed=0):\n",
    "    np.random.seed(seed)\n",
    "    n_samples = 30\n",
    "    X = np.sort(np.random.rand(n_samples))\n",
    "    y = np.sin(2 * np.pi * X) + np.random.randn(n_samples) * 0.1\n",
    "    X = X[:, np.newaxis]\n",
    "    return (X, y)\n",
    "\n",
    "X, y = make_nonlinear()\n",
    "dfX = pd.DataFrame(X, columns=[\"x\"])\n",
    "dfX = sm.add_constant(dfX)\n",
    "dfy = pd.DataFrame(y, columns=[\"y\"])\n",
    "df = pd.concat([dfX, dfy], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intercept       -0.169863\n",
       "x               25.735773\n",
       "I(x ** 2)     -428.141683\n",
       "I(x ** 3)     3866.723113\n",
       "I(x ** 4)   -18340.939658\n",
       "I(x ** 5)    49326.072530\n",
       "I(x ** 6)   -78884.743049\n",
       "I(x ** 7)    74538.645130\n",
       "I(x ** 8)   -38453.132179\n",
       "I(x ** 9)     8350.254983\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sm.OLS.from_formula(\n",
    "     \"y ~ x + I(x**2) + I(x**3) + I(x**4) + I(x**5) + I(x**6) + I(x**7) + I(x**8) + I(x**9)\", data=df)\n",
    "\n",
    "result1 = model.fit()\n",
    "result1.params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
